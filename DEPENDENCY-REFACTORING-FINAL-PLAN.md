# ÏùòÏ°¥ÏÑ± Í¥ÄÎ¶¨ Î∞è Î¶¨Ìå©ÌÜ†ÎßÅ ÏïàÏ†ÑÏÑ± ÏµúÏ¢Ö Í∞úÏÑ†Í≥ÑÌöç
**Event-Driven Architecture + Gap Analysis Integration + Senior Review**

**ÏûëÏÑ±Ïùº**: 2025-10-17 (ÏµúÏ¢Ö ÏóÖÎç∞Ïù¥Ìä∏: 2025-10-17)
**ÌîÑÎ°úÏ†ùÌä∏**: Math Education Multi-Agent System
**Î™©Ìëú**: Event-driven ÏïÑÌÇ§ÌÖçÏ≤ò Í∏∞Î∞ò Î¶¨Ìå©ÌÜ†ÎßÅ ÏïàÏ†ÑÏÑ± Î≥¥Ïû• Î∞è ÎåÄÍ∑úÎ™® Î≥ÄÍ≤Ω ÏûêÎèôÌôî
**ÏÉÅÌÉú**: ‚úÖ Senior Developer Review Approved with Revisions

---

## üìä Executive Summary

### Î¨∏ÏÑú ÎπÑÍµê Î∂ÑÏÑù Í≤∞Í≥º

**v2.0 Í≥ÑÌöç** (`DEPENDENCY-REFACTORING-IMPROVEMENT-PLAN.md`)
- ClaudeSDKClient + Task delegation Í∏∞Î∞ò
- 7Í∞ú Phase (Phase 0-6)
- Extended Thinking, Memory Tool, Skills, Hooks, Parallel Execution, Infinite Loop
- **Î¨∏Ï†ú**: Ïã§Ï†ú ÏΩîÎìúÎ≤†Ïù¥Ïä§ ÏïÑÌÇ§ÌÖçÏ≤òÏôÄ Í∑ºÎ≥∏Ï†Å Î∂àÏùºÏπò

**v3.0 Í≥ÑÌöç** (`DEPENDENCY-REFACTORING-IMPROVEMENT-PLAN-v3.md`)
- EventRouter + Handlers Í∏∞Î∞ò (event-driven)
- 7Í∞ú Phase (Phase 1-7)
- Í∏∞Ï°¥ Ïù∏ÌîÑÎùº Ïû¨ÏÇ¨Ïö© (DependencyAgent, EventRouter, Observability)
- **Í∞úÏÑ†**: Ïã§Ï†ú ÏïÑÌÇ§ÌÖçÏ≤ò Î∞òÏòÅ, 14Ïùº Íµ¨ÌòÑ ÏòàÏÉÅ

**Gap Analysis** (`GAP-ANALYSIS-SUMMARY.md`)
- ÏïÑÌÇ§ÌÖçÏ≤ò Î∂àÏùºÏπò ÏÉÅÏÑ∏ Î∂ÑÏÑù
- 7Í∞ú Ïû†Ïû¨Ï†Å Ïù¥Ïäà ÏãùÎ≥Ñ (race condition, cache invalidation, hook overhead Îì±)
- 4Í∞ú Ï∂îÍ∞Ä Í∞úÏÑ† Ï†úÏïà (Baseline metrics, Lock manager, Chaos tests Îì±)
- **Impact**: v3.0Ïóê ÎàÑÎùΩÎêú Î¶¨Ïä§ÌÅ¨ ÎåÄÏùë Î∞è Í∞úÏÑ†ÏÇ¨Ìï≠ Î≥¥ÏôÑ

### ÏµúÏ¢Ö Í≥ÑÌöç Î∞©Ìñ•

**Ïù¥ Î¨∏ÏÑúÎäî v3.0 + Gap Analysis + Senior Developer ReviewÎ•º ÌÜµÌï©Ìïú ÌîÑÎ°úÎçïÏÖò Ï§ÄÎπÑ Ïã§Ìñâ Í≥ÑÌöçÏûÖÎãàÎã§.**

ÌïµÏã¨ ÏõêÏπô:
1. ‚úÖ **Í∏∞Ï°¥ Ïù∏ÌîÑÎùº ÏµúÎåÄ ÌôúÏö©** (DependencyAgent 535L, EventRouter, Observability)
2. ‚úÖ **Event-driven Ìå®ÌÑ¥ Ï§ÄÏàò** (ClaudeSDKClient ÏùòÏ°¥ Ï†úÍ±∞)
3. ‚úÖ **Î¶¨Ïä§ÌÅ¨ ÎåÄÏùë Í∞ïÌôî** (Lock manager, Cache invalidation, Event aggregation)
4. ‚úÖ **Îã®Í≥ÑÎ≥Ñ Í≤ÄÏ¶ù** (Phase 0 baseline ‚Üí Íµ¨ÌòÑ ‚Üí E2E test)
5. ‚úÖ **ÌîÑÎ°úÎçïÏÖò ÏïàÏ†ÑÏÑ±** (Rollback, Feature flags, Performance SLOs)
6. ‚úÖ **Î∂ÑÏÇ∞ ÌôòÍ≤Ω ÏßÄÏõê** (File-based locks, CI/CD Ìò∏Ìôò)

---

## üéØ Phase 0: Baseline Metrics & Critical Path Ï†ïÏùò

**ÏÜåÏöî ÏãúÍ∞Ñ**: 1Ïùº
**Î™©Ìëú**: Î¶¨Ìå©ÌÜ†ÎßÅ Ìö®Í≥º Ï∏°Ï†ï Í∏∞Ï§ÄÏÑ† ÏÑ§Ï†ï Î∞è Critical Path Î™ÖÌôïÌôî
**Ïö∞ÏÑ†ÏàúÏúÑ**: üî• Critical (Î™®Îì† PhaseÏùò Í∏∞Î∞ò)

### 0.1 ÏùòÏ°¥ÏÑ± Í∑∏ÎûòÌîÑ ÏÉùÏÑ± Î∞è ÏãúÍ∞ÅÌôî

**Íµ¨ÌòÑ ÏúÑÏπò**: `scripts/generate_dependency_report.py`

```python
"""
Dependency Baseline Report Generator
Uses DependencyAgent to create initial metrics and visualization
"""
from lib.dependency_analyzer import DependencyAgent
import networkx as nx
from pyvis.network import Network
import json
from datetime import datetime

def generate_baseline_report():
    """Generate dependency baseline metrics and HTML visualization"""

    # 1. Build dependency graph
    agent = DependencyAgent()
    graph = agent.build_and_cache_graph()

    # 2. Calculate metrics
    metrics = {
        "total_nodes": graph.number_of_nodes(),
        "total_edges": graph.number_of_edges(),
        "timestamp": datetime.now().isoformat(),
        "git_commit": agent.get_current_commit_hash()
    }

    # 3. Calculate coupling metrics per module
    coupling_metrics = {}
    for node in graph.nodes():
        afferent = len(list(graph.predecessors(node)))  # incoming
        efferent = len(list(graph.successors(node)))     # outgoing
        instability = efferent / (afferent + efferent) if (afferent + efferent) > 0 else 0

        coupling_metrics[node] = {
            "afferent": afferent,
            "efferent": efferent,
            "instability": instability
        }

    # 4. Detect circular dependencies
    cycles = list(nx.simple_cycles(graph))
    metrics["circular_dependencies"] = len(cycles)
    metrics["cycle_details"] = cycles[:10]  # First 10

    # 5. Critical path analysis
    critical_nodes = [n for n, data in graph.nodes(data=True) if data.get("is_critical")]
    metrics["critical_nodes_count"] = len(critical_nodes)

    # 6. Generate HTML visualization
    net = Network(height="900px", width="100%", directed=True)

    # Add nodes with color coding
    for node, data in graph.nodes(data=True):
        color = "red" if data.get("is_critical") else "lightblue"
        net.add_node(node, label=node, color=color)

    # Add edges
    for source, target in graph.edges():
        net.add_edge(source, target)

    net.save_graph("reports/dependency_report.html")

    # 7. Save metrics to Markdown
    with open("reports/DEPENDENCY_BASELINE_REPORT.md", "w") as f:
        f.write(f"# Dependency Baseline Report\n\n")
        f.write(f"**Generated**: {metrics['timestamp']}\n")
        f.write(f"**Git Commit**: {metrics['git_commit']}\n\n")
        f.write(f"## Summary\n")
        f.write(f"- Total Nodes: {metrics['total_nodes']}\n")
        f.write(f"- Total Edges: {metrics['total_edges']}\n")
        f.write(f"- Circular Dependencies: {metrics['circular_dependencies']}\n")
        f.write(f"- Critical Nodes: {metrics['critical_nodes_count']}\n\n")
        f.write(f"## Top 10 Most Coupled Modules\n")
        sorted_modules = sorted(coupling_metrics.items(), key=lambda x: x[1]["instability"], reverse=True)
        for module, data in sorted_modules[:10]:
            f.write(f"- `{module}`: Instability={data['instability']:.2f}, In={data['afferent']}, Out={data['efferent']}\n")
        f.write(f"\n## Circular Dependencies (First 10)\n")
        for i, cycle in enumerate(cycles[:10], 1):
            f.write(f"{i}. {' ‚Üí '.join(cycle)} ‚Üí {cycle[0]}\n")

    print(f"‚úÖ Baseline report generated:")
    print(f"   - HTML: reports/dependency_report.html")
    print(f"   - Metrics: reports/DEPENDENCY_BASELINE_REPORT.md")

if __name__ == "__main__":
    generate_baseline_report()
```

### 0.2 Critical Path Ï†ïÏùò Ïô∏Î∂ÄÌôî

**Î¨∏Ï†ú (Gap Analysis Issue 6)**:
- `lib/dependency_analyzer.py:233`Ïóê critical componentsÍ∞Ä ÌïòÎìúÏΩîÎî©Îê®
- Ïã§Ï†ú ÌååÏùºÎ™ÖÍ≥º Î∂àÏùºÏπò ("knowledge-builder" vs "file_builder_agent.py")

**Ìï¥Í≤∞**:
```json
// .claude/config/critical_paths.json
{
  "patterns": [
    "main.py",
    "containers.py",
    "lib/event_router.py",
    "handlers/**/*.py",
    "infrastructure/**/*.py",
    ".claude/hooks/**/*.py",
    "subagents/__init__.py",
    "generate_agent_rules.py"
  ],
  "explicit_files": [
    "subagents/file_builder_agent.py",
    "subagents/validator_agent.py",
    "subagents/meta_orchestrator_agent.py"
  ],
  "description": "Critical paths requiring manual review for all changes"
}
```

**DependencyAgent ÏàòÏ†ï**:
```python
# lib/dependency_analyzer.py (line 230 Í∑ºÏ≤ò)
import json
from pathlib import Path

def _load_critical_paths(self) -> Set[str]:
    """Load critical paths from JSON config"""
    config_path = Path(".claude/config/critical_paths.json")
    if not config_path.exists():
        return self._default_critical_components()

    with open(config_path) as f:
        config = json.load(f)

    critical_set = set(config.get("explicit_files", []))

    # Pattern matching
    for pattern in config.get("patterns", []):
        matched_files = Path(".").glob(pattern)
        critical_set.update(str(f) for f in matched_files)

    return critical_set

def __init__(self, base_path: str = "."):
    # ... existing code ...
    self.critical_components = self._load_critical_paths()
```

### 0.3 Baseline Comparison & Tracking

**Íµ¨ÌòÑ ÏúÑÏπò**: `scripts/generate_dependency_report.py` (mode Ï∂îÍ∞Ä)

```python
def generate_baseline_report(mode="snapshot"):
    """
    mode:
      - snapshot: Îã®Ïùº Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± (Ï≤´ Ïã§Ìñâ)
      - compare: Ïù¥Ï†Ñ baselineÍ≥º ÎπÑÍµê
      - track: Git hookÏúºÎ°ú ÏûêÎèô Ï∂îÏ†Å
    """
    agent = DependencyAgent()
    graph = agent.build_and_cache_graph()
    
    if mode == "compare":
        # Load previous baseline
        old_metrics = load_previous_metrics("reports/DEPENDENCY_BASELINE_REPORT.md")
        new_metrics = calculate_metrics(graph)
        
        # Generate comparison report
        print(f"\nüìä Metrics Comparison:")
        print(f"  Total Nodes: {old_metrics['nodes']} ‚Üí {new_metrics['nodes']} ({delta(old_metrics['nodes'], new_metrics['nodes'])})")
        print(f"  Circular Deps: {old_metrics['cycles']} ‚Üí {new_metrics['cycles']} ({delta(old_metrics['cycles'], new_metrics['cycles'])})")
        print(f"  Avg Instability: {old_metrics['instability']:.2f} ‚Üí {new_metrics['instability']:.2f}")
        
        # Save comparison
        with open("reports/DEPENDENCY_COMPARISON.md", "w") as f:
            f.write(generate_comparison_markdown(old_metrics, new_metrics))
    
    elif mode == "track":
        # Auto-track on git commit (via hook)
        save_snapshot_with_git_hash(graph, agent.get_current_commit_hash())
```

**Git Hook Integration**: `.git/hooks/post-commit`
```bash
#!/bin/bash
# Auto-generate baseline comparison on commit
uv run scripts/generate_dependency_report.py --mode=compare
```

### 0.4 Deliverables

‚úÖ `reports/dependency_report.html` (pyvis Ïù∏ÌÑ∞ÎûôÌã∞Î∏å ÏãúÍ∞ÅÌôî)
‚úÖ `reports/DEPENDENCY_BASELINE_REPORT.md` (metrics + circular deps)
‚úÖ `reports/DEPENDENCY_COMPARISON.md` (Ïù¥Ï†Ñ vs ÌòÑÏû¨ ÎπÑÍµê)
‚úÖ `.claude/config/critical_paths.json` (Ïô∏Î∂ÄÌôîÎêú critical path Ï†ïÏùò)
‚úÖ `.claude/config/critical_paths.local.json` (gitignored, Í∞úÏù∏ override)
‚úÖ `lib/dependency_analyzer.py` (JSON Í∏∞Î∞ò critical path loading)
‚úÖ `.git/hooks/post-commit` (ÏûêÎèô tracking)

---

## üéØ Phase 1: Event-Driven Dependency Analysis

**ÏÜåÏöî ÏãúÍ∞Ñ**: 2Ïùº
**Î™©Ìëú**: DependencyAgentÎ•º EventRouterÏóê ÌÜµÌï©
**Ïö∞ÏÑ†ÏàúÏúÑ**: üî• High

### 1.1 DependencyCheckHandler Íµ¨ÌòÑ

**Íµ¨ÌòÑ ÏúÑÏπò**: `handlers/dependency_check_handler.py`

```python
"""
Dependency Check Handler - Event-driven dependency analysis
Listens to FILE_MODIFIED events and emits impact analysis
"""
from lib.event_router import EventRouter
from lib.dependency_analyzer import DependencyAgent
from typing import Dict, Any
import asyncio

class DependencyCheckHandler:
    """
    Event flow:
    FILE_MODIFIED ‚Üí analyze ‚Üí DEPENDENCY_IMPACT_DETECTED (or CRITICAL_DEPENDENCY_ALERT)
    """

    def __init__(self, event_router: EventRouter):
        self.event_router = event_router
        self.dependency_agent = None  # Lazy init (Gap Analysis Issue 3)
        self._init_lock = asyncio.Lock()

        # Subscribe to file modification events
        event_router.subscribe("FILE_MODIFIED", self.on_file_modified)

    async def _ensure_agent_initialized(self):
        """Lazy initialization to avoid startup overhead (Gap Analysis Issue 4)"""
        if self.dependency_agent is None:
            async with self._init_lock:
                if self.dependency_agent is None:
                    self.dependency_agent = DependencyAgent()
                    # Warm up cache in background
                    asyncio.create_task(self.dependency_agent.build_and_cache_graph())

    async def on_file_modified(self, data: Dict[str, Any]):
        """Handle file modification event"""
        await self._ensure_agent_initialized()

        file_path = data.get("file_path")
        batch_id = data.get("batch_id")  # For aggregation (Gap Analysis Issue 5)

        # Check if working directory is clean (Gap Analysis Issue 2)
        if not self._is_working_dir_clean():
            # Force rebuild without cache
            self.dependency_agent.build_and_cache_graph(force=True)

        # Analyze impact
        impact_nodes = self.dependency_agent.get_impact_set([file_path], depth=2)
        critical_affected = any(node.is_critical for node in impact_nodes)

        # Publish impact event
        await self.event_router.publish("DEPENDENCY_IMPACT_DETECTED", {
            "file_path": file_path,
            "impact_size": len(impact_nodes),
            "critical_affected": critical_affected,
            "impact_nodes": [node.node_id for node in impact_nodes[:10]],
            "batch_id": batch_id  # For aggregation
        })

        # If critical, also publish alert
        if critical_affected:
            await self.event_router.publish("CRITICAL_DEPENDENCY_ALERT", {
                "file_path": file_path,
                "impact_nodes": [n.node_id for n in impact_nodes if n.is_critical],
                "batch_id": batch_id
            })

    def _is_working_dir_clean(self) -> bool:
        """Check if working directory has uncommitted changes (Gap Analysis Issue 2)"""
        import subprocess
        result = subprocess.run(
            ["git", "status", "--porcelain"],
            capture_output=True,
            text=True
        )
        return len(result.stdout.strip()) == 0
```

### 1.2 Container Integration

**Íµ¨ÌòÑ ÏúÑÏπò**: `containers.py` (ÌôïÏû•)

```python
# Add to Container class
from handlers.dependency_check_handler import DependencyCheckHandler

class Container(containers.DeclarativeContainer):
    # ... existing code ...

    # Dependency Analysis Handler
    dependency_check_handler = providers.Singleton(
        DependencyCheckHandler,
        event_router=event_router
    )
```

### 1.3 Hook Integration (Event Emission + Blocking)

**Íµ¨ÌòÑ ÏúÑÏπò**: `.claude/hooks/pre_tool_use.py` (ÌôïÏû•)

```python
"""
Extended pre_tool_use hook with event emission and critical path blocking
Addresses Senior Review Critical Issue #1: Async Error Propagation
"""
import sys
import json
from pathlib import Path
from typing import Dict, Any, Optional
import asyncio

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

async def on_edit_file(event_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Emit FILE_MODIFIED event when Edit tool is used"""
    tool_name = event_data.get('tool_name', '')
    tool_input = event_data.get('tool_input', {})

    if tool_name != "Edit":
        return None

    file_path = tool_input.get('file_path', '')
    if not file_path.endswith('.py'):
        return None  # Only Python files

    # Initialize container
    from containers import Container
    from lib.dependency_analyzer import DependencyAgent
    
    container = Container()
    event_router = container.event_router()
    
    # Quick check: Is this a critical file?
    agent = DependencyAgent()
    is_critical = agent.is_critical_component(file_path)
    
    if is_critical:
        # BLOCKING path for critical files (Senior Review Issue #1)
        try:
            # Synchronous analysis before allowing edit
            impact_nodes = agent.get_impact_set([file_path], depth=2)
            impact_size = len(impact_nodes)
            
            # Emit event for tracking (fire-and-forget)
            asyncio.create_task(event_router.publish("FILE_MODIFIED", {
                "file_path": file_path,
                "source": "pre_tool_use_hook",
                "critical": True,
                "impact_size": impact_size
            }))
            
            # Block with warning if impact is high
            if impact_size > 50:
                return {
                    "block": True,
                    "message": f"""
‚ö†Ô∏è  CRITICAL PATH WITH HIGH IMPACT
File: {file_path}
Impact: {impact_size} nodes affected

This change affects critical infrastructure with broad impact.

Recommended actions:
1. Review impact analysis:
   uv run .claude/skills/refactoring-safety/scripts/analyze_impact.py {file_path}

2. Get AI recommendations (Extended Thinking):
   Wait for REFACTORING_RECOMMENDATION event

3. Execute with proper workflow:
   Follow Refactoring Safety Skill guidelines

To override: Set FF_AUTO_BLOCKING=false
                    """
                }
            
        except Exception as e:
            # Don't block on analysis failure
            print(f"‚ö†Ô∏è  Critical path check failed: {e}", file=sys.stderr)
    
    else:
        # NON-BLOCKING path for non-critical files
        # Fire-and-forget event emission
        asyncio.create_task(event_router.publish("FILE_MODIFIED", {
            "file_path": file_path,
            "source": "pre_tool_use_hook",
            "critical": False
        }))
    
    return None  # Allow edit to proceed

# Hook entry point
if __name__ == "__main__":
    event_data = json.loads(sys.stdin.read())
    result = asyncio.run(on_edit_file(event_data))

    if result:
        print(json.dumps(result))
        sys.exit(1 if result.get("block") else 0)
```

**Feature Flag**: `.env` or environment variable
```bash
# Disable auto-blocking for development
FF_AUTO_BLOCKING=false

# Enable in CI/CD
FF_AUTO_BLOCKING=true
```

### 1.4 Deliverables

‚úÖ `handlers/dependency_check_handler.py` (Event-driven analysis)
‚úÖ `containers.py` (Handler registration)
‚úÖ `.claude/hooks/pre_tool_use.py` (Event emission)
‚úÖ Unit tests: `tests/test_dependency_check_handler.py`

---

## üéØ Phase 2: Extended Thinking for Critical Changes

**ÏÜåÏöî ÏãúÍ∞Ñ**: 2Ïùº
**Î™©Ìëú**: AI-assisted refactoring planning for critical path changes
**Ïö∞ÏÑ†ÏàúÏúÑ**: üü° Medium

### 2.1 ExtendedThinkingHandler Íµ¨ÌòÑ (with Caching & Fallback)

**Íµ¨ÌòÑ ÏúÑÏπò**: `handlers/extended_thinking_handler.py`

```python
"""
Extended Thinking Handler - Complex dependency reasoning with caching
Addresses Senior Review Critical Issue #3: API Cost/Latency
"""
from claude_agent_sdk import ClaudeSDKClient, ClaudeAgentOptions
from lib.event_router import EventRouter
from typing import Dict, Any, Optional
import asyncio
import hashlib
from cachetools import TTLCache
from datetime import datetime

class ExtendedThinkingHandler:
    """
    Event flow:
    CRITICAL_DEPENDENCY_ALERT ‚Üí Extended Thinking analysis ‚Üí REFACTORING_RECOMMENDATION
    
    Features:
    - TTL cache (5Î∂Ñ) for repeated analyses
    - Fallback to rule-based recommendations on API failure
    - Cost tracking
    """

    def __init__(self, event_router: EventRouter, enable_thinking: bool = True):
        self.event_router = event_router
        self.enable_thinking = enable_thinking  # Feature flag
        
        # TTL Cache: 5Î∂Ñ ÎèôÏïà ÎèôÏùº Î∂ÑÏÑù Í≤∞Í≥º Ïû¨ÏÇ¨Ïö©
        self.recommendation_cache = TTLCache(maxsize=100, ttl=300)
        
        # Cost tracking
        self.api_calls_count = 0
        self.total_cost_usd = 0.0

        # Subscribe to critical dependency alerts only
        event_router.subscribe("CRITICAL_DEPENDENCY_ALERT", self.analyze_with_thinking)

    def _generate_cache_key(self, file_path: str, impact_nodes: list) -> str:
        """Generate cache key from file path and impact nodes"""
        nodes_str = ','.join(sorted(impact_nodes))
        content = f"{file_path}:{nodes_str}"
        return hashlib.md5(content.encode()).hexdigest()

    async def _generate_fallback_recommendation(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Rule-based fallback when API is unavailable"""
        file_path = data["file_path"]
        impact_nodes = data["impact_nodes"]
        
        # Simple rule-based logic
        risk = "HIGH" if len(impact_nodes) > 10 else "MEDIUM"
        
        return {
            "risk": risk,
            "safe_sequence": [
                "1. Create backup branch",
                "2. Run full test suite",
                "3. Review all import statements",
                "4. Update incrementally",
                "5. Verify after each step"
            ],
            "reasoning": "Rule-based fallback recommendation (Extended Thinking unavailable)",
            "alternatives": ["Consider dependency injection", "Extract interface"],
            "fallback": True
        }

    async def analyze_with_thinking(self, data: Dict[str, Any]):
        """Use Extended Thinking for complex dependency reasoning"""
        file_path = data["file_path"]
        impact_nodes = data["impact_nodes"]
        batch_id = data.get("batch_id")
        
        # Check feature flag
        if not self.enable_thinking:
            result = await self._generate_fallback_recommendation(data)
            await self._publish_recommendation(file_path, result, batch_id, cached=False)
            return
        
        # Check cache
        cache_key = self._generate_cache_key(file_path, impact_nodes)
        
        if cache_key in self.recommendation_cache:
            result = self.recommendation_cache[cache_key]
            await self._publish_recommendation(file_path, result, batch_id, cached=True)
            return

        # API call with fallback
        try:
            result = await self._call_extended_thinking_api(file_path, impact_nodes)
            
            # Update metrics
            self.api_calls_count += 1
            self.total_cost_usd += 0.03  # Estimated cost per call
            
            # Cache result
            self.recommendation_cache[cache_key] = result
            
        except (TimeoutError, Exception) as e:
            print(f"‚ö†Ô∏è  Extended Thinking API failed: {e}", file=sys.stderr)
            result = await self._generate_fallback_recommendation(data)
        
        await self._publish_recommendation(file_path, result, batch_id, cached=False)

    async def _call_extended_thinking_api(self, file_path: str, impact_nodes: list) -> Dict[str, Any]:
        """Call Claude API with timeout"""
        options = ClaudeAgentOptions(
            model="claude-sonnet-4-5-20250929",
            thinking={"type": "enabled", "budget_tokens": 2048},
            betas=["interleaved-thinking-2025-05-14"]
        )

        async with ClaudeSDKClient(options) as client:
            prompt = f"""
            Analyze refactoring impact for CRITICAL PATH change:

            File: {file_path}
            Critical nodes affected: {', '.join(impact_nodes)}

            Generate:
            1. Risk assessment (HIGH/MEDIUM/LOW)
            2. Safe refactoring sequence (step-by-step)
            3. Reasoning for recommendations
            4. Alternative approaches (if any)

            Output JSON format:
            {{
              "risk": "HIGH|MEDIUM|LOW",
              "safe_sequence": ["Step 1", "Step 2", ...],
              "reasoning": "Extended thinking output",
              "alternatives": ["Alt 1", "Alt 2"]
            }}
            """

            # Timeout: 15 seconds max
            response = await asyncio.wait_for(
                client.query(prompt),
                timeout=15.0
            )
            return response.get("content", {})

    async def _publish_recommendation(self, file_path: str, result: Dict, batch_id: str, cached: bool):
        """Publish recommendation event"""
        await self.event_router.publish("REFACTORING_RECOMMENDATION", {
            "file_path": file_path,
            "risk": result.get("risk", "UNKNOWN"),
            "sequence": result.get("safe_sequence", []),
            "reasoning": result.get("reasoning", ""),
            "alternatives": result.get("alternatives", []),
            "batch_id": batch_id,
            "cached": cached,
            "fallback": result.get("fallback", False),
            "timestamp": datetime.now().isoformat()
        })
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get cost/usage metrics"""
        return {
            "api_calls": self.api_calls_count,
            "total_cost_usd": self.total_cost_usd,
            "cache_size": len(self.recommendation_cache),
            "cache_hit_rate": self._calculate_cache_hit_rate()
        }
```

**Feature Flag Configuration**: `.env`
```bash
# Enable/disable Extended Thinking
FF_EXTENDED_THINKING=true

# Set to false for local development to save costs
FF_EXTENDED_THINKING=false
```

### 2.2 Container Integration

```python
# containers.py
extended_thinking_handler = providers.Singleton(
    ExtendedThinkingHandler,
    event_router=event_router
)
```

### 2.3 Deliverables

‚úÖ `handlers/extended_thinking_handler.py`
‚úÖ `containers.py` (Handler registration)
‚úÖ Unit tests: `tests/test_extended_thinking_handler.py`

---

## üéØ Phase 3: Refactoring Lock Manager (Gap Analysis + Senior Review)

**ÏÜåÏöî ÏãúÍ∞Ñ**: 2Ïùº (file-based backend Ï∂îÍ∞Ä)
**Î™©Ìëú**: Î≥ëÎ†¨ Î¶¨Ìå©ÌÜ†ÎßÅ Ïãú Í≤ΩÏüÅ Ï°∞Í±¥ Î∞©ÏßÄ (Î∂ÑÏÇ∞ ÌôòÍ≤Ω ÏßÄÏõê)
**Ïö∞ÏÑ†ÏàúÏúÑ**: üî• High (Gap Analysis Issue 1 + Senior Review Issue #2)

### 3.1 RefactoringLockManager Íµ¨ÌòÑ (Multi-Backend)

**Íµ¨ÌòÑ ÏúÑÏπò**: `lib/refactoring_lock_manager.py`

```python
"""
Refactoring Lock Manager - Prevent concurrent file modifications
Addresses Senior Review Critical Issue #2: Distributed Environment Support

Supports multiple backends:
- memory: In-process asyncio.Lock (Îã®Ïùº ÌîÑÎ°úÏÑ∏Ïä§, Í∞úÎ∞úÏö©)
- file: OS-level fcntl locks (CI/CD, Îã§Ï§ë ÌîÑÎ°úÏÑ∏Ïä§)
"""
import asyncio
import fcntl
import os
from typing import Dict, Optional
from datetime import datetime, timedelta
from pathlib import Path
from abc import ABC, abstractmethod

class LockBackend(ABC):
    """Abstract lock backend"""
    
    @abstractmethod
    async def acquire(self, file_path: str, owner: str) -> str:
        """Acquire lock, returns token"""
        pass
    
    @abstractmethod
    def release(self, token: str):
        """Release lock by token"""
        pass

class MemoryLockBackend(LockBackend):
    """In-memory lock backend (asyncio.Lock)"""
    
    def __init__(self, ttl_seconds: int = 30):
        self.locks: Dict[str, asyncio.Lock] = {}
        self.lock_metadata: Dict[str, Dict] = {}
        self.ttl_seconds = ttl_seconds
    
    async def acquire(self, file_path: str, owner: str) -> str:
        # Create lock if not exists
        if file_path not in self.locks:
            self.locks[file_path] = asyncio.Lock()

        # Acquire lock (blocks if already held)
        await self.locks[file_path].acquire()

        # Generate token
        token = f"{file_path}:{owner}:{datetime.now().timestamp()}"

        # Store metadata
        self.lock_metadata[token] = {
            "file_path": file_path,
            "owner": owner,
            "acquired_at": datetime.now(),
            "expires_at": datetime.now() + timedelta(seconds=self.ttl_seconds)
        }

        # Schedule TTL cleanup
        asyncio.create_task(self._schedule_cleanup(token))

        return token
    
    def release(self, token: str):
        if token not in self.lock_metadata:
            raise ValueError(f"Invalid token: {token}")

        metadata = self.lock_metadata[token]
        file_path = metadata["file_path"]

        # Release lock
        if file_path in self.locks and self.locks[file_path].locked():
            self.locks[file_path].release()

        # Remove metadata
        del self.lock_metadata[token]
    
    async def _schedule_cleanup(self, token: str):
        """Cleanup expired locks (TTL enforcement)"""
        await asyncio.sleep(self.ttl_seconds)

        if token in self.lock_metadata:
            metadata = self.lock_metadata[token]
            if datetime.now() > metadata["expires_at"]:
                print(f"‚ö†Ô∏è  Lock expired and force-released: {metadata['file_path']}")
                self.release(token)

class FileLockBackend(LockBackend):
    """
    File-based lock backend (fcntl)
    Supports multiple processes (CI/CD environments)
    """
    
    def __init__(self, lock_dir: str = "/tmp/refactoring-locks", ttl_seconds: int = 30):
        self.lock_dir = Path(lock_dir)
        self.lock_dir.mkdir(parents=True, exist_ok=True)
        self.ttl_seconds = ttl_seconds
        self.file_handles: Dict[str, int] = {}  # token -> file descriptor
    
    async def acquire(self, file_path: str, owner: str) -> str:
        """Acquire OS-level file lock"""
        # Generate lock file path
        import hashlib
        file_hash = hashlib.md5(file_path.encode()).hexdigest()
        lock_file = self.lock_dir / f"{file_hash}.lock"
        
        # Open lock file
        fd = os.open(str(lock_file), os.O_CREAT | os.O_RDWR)
        
        # Acquire exclusive lock (blocks until available)
        fcntl.flock(fd, fcntl.LOCK_EX)
        
        # Write metadata
        metadata = f"{file_path}|{owner}|{datetime.now().isoformat()}\n"
        os.write(fd, metadata.encode())
        
        # Generate token
        token = f"{file_path}:{owner}:{datetime.now().timestamp()}"
        
        # Store file descriptor
        self.file_handles[token] = fd
        
        # Schedule TTL cleanup
        asyncio.create_task(self._schedule_cleanup(token, lock_file))
        
        return token
    
    def release(self, token: str):
        """Release file lock"""
        if token not in self.file_handles:
            raise ValueError(f"Invalid token: {token}")
        
        fd = self.file_handles[token]
        
        # Release lock and close file
        fcntl.flock(fd, fcntl.LOCK_UN)
        os.close(fd)
        
        # Remove from handles
        del self.file_handles[token]
    
    async def _schedule_cleanup(self, token: str, lock_file: Path):
        """Cleanup expired locks"""
        await asyncio.sleep(self.ttl_seconds)
        
        if token in self.file_handles:
            print(f"‚ö†Ô∏è  Lock expired and force-released: {token}")
            self.release(token)
            # Remove lock file
            lock_file.unlink(missing_ok=True)

class RefactoringLockManager:
    """
    Centralized lock manager with pluggable backends
    
    Usage:
        # In-memory (development)
        manager = RefactoringLockManager(backend="memory")
        
        # File-based (CI/CD)
        manager = RefactoringLockManager(backend="file")
    """

    def __init__(self, backend: str = "memory", ttl_seconds: int = 30):
        if backend == "memory":
            self.backend = MemoryLockBackend(ttl_seconds)
        elif backend == "file":
            self.backend = FileLockBackend(ttl_seconds=ttl_seconds)
        else:
            raise ValueError(f"Unknown backend: {backend}")

    async def acquire_lock(self, file_path: str, owner: str = "unknown") -> str:
        """
        Acquire lock for file with TTL and heartbeat
        Returns: lock_token for release
        """
        return await self.backend.acquire(file_path, owner)

    def release_lock(self, token: str):
        """Release lock by token"""
        self.backend.release(token)

    async def heartbeat(self, token: str):
        """Extend TTL by sending heartbeat"""
        # Heartbeat support (implementation depends on backend)
        if isinstance(self.backend, MemoryLockBackend):
            if token not in self.backend.lock_metadata:
                raise ValueError(f"Invalid token: {token}")
            self.backend.lock_metadata[token]["expires_at"] = \
                datetime.now() + timedelta(seconds=self.backend.ttl_seconds)
```

**Environment Configuration**: `.env`
```bash
# Lock backend selection
LOCK_BACKEND=file  # or "memory"

# Lock TTL (seconds)
LOCK_TTL_SECONDS=30
```

### 3.2 Container Integration

```python
# containers.py
refactoring_lock_manager = providers.Singleton(
    RefactoringLockManager,
    ttl_seconds=30
)
```

### 3.3 Usage Example

```python
# In batch refactoring handler
lock_manager = container.refactoring_lock_manager()

async def safe_modify_file(file_path: str):
    """Safely modify file with lock"""
    token = await lock_manager.acquire_lock(file_path, owner="batch_refactor")
    try:
        # Modify file
        with open(file_path, 'w') as f:
            f.write(new_content)
    finally:
        lock_manager.release_lock(token)
```

### 3.4 Deliverables

‚úÖ `lib/refactoring_lock_manager.py`
‚úÖ `containers.py` (Singleton registration)
‚úÖ Unit tests: `tests/test_refactoring_lock_manager.py`

---

## üéØ Phase 4: Refactoring Safety Skill

**ÏÜåÏöî ÏãúÍ∞Ñ**: 3Ïùº
**Î™©Ìëú**: Progressive disclosure skill with CLI wrappers and workflows
**Ïö∞ÏÑ†ÏàúÏúÑ**: üü° Medium

### 4.1 Skill Directory Structure

```
.claude/skills/refactoring-safety/
‚îú‚îÄ‚îÄ SKILL.md                    # Level 1+2 (<5k tokens)
‚îú‚îÄ‚îÄ reference/
‚îÇ   ‚îú‚îÄ‚îÄ event-integration.md    # Event-driven patterns
‚îÇ   ‚îú‚îÄ‚îÄ dependency-analysis.md  # DependencyAgent usage
‚îÇ   ‚îú‚îÄ‚îÄ lock-manager.md         # Concurrent refactoring guide
‚îÇ   ‚îî‚îÄ‚îÄ safe-workflows.md       # Step-by-step guides
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ analyze_impact.py       # CLI wrapper for DependencyAgent
‚îÇ   ‚îú‚îÄ‚îÄ emit_refactoring_event.py  # Programmatic event emission
‚îÇ   ‚îú‚îÄ‚îÄ safe_rename.py          # Bowler-based rename (future)
‚îÇ   ‚îî‚îÄ‚îÄ watch_dependencies.py   # File watcher integration
‚îî‚îÄ‚îÄ resources/
    ‚îî‚îÄ‚îÄ refactoring_templates/
        ‚îú‚îÄ‚îÄ rename_module.yaml
        ‚îú‚îÄ‚îÄ extract_interface.yaml
        ‚îî‚îÄ‚îÄ move_function.yaml
```

### 4.2 SKILL.md

```yaml
---
name: Refactoring Safety Skill
description: Event-driven refactoring with dependency analysis, lock management, and AI recommendations. Use when renaming modules, moving files, or changing function signatures in critical paths.
---

# Refactoring Safety Skill

## Quick Start

### CLI Analysis
```bash
# Analyze impact of changing a file
uv run .claude/skills/refactoring-safety/scripts/analyze_impact.py <file_path>
```

**Output**:
- Direct dependents (who imports this?)
- Dependencies (what does this import?)
- Critical path flag (‚ö†Ô∏è if true)
- Blast radius (affected nodes count)

### Event-Driven Analysis
```python
from containers import Container

container = Container()
event_router = container.event_router()

# Trigger analysis
await event_router.publish("FILE_MODIFIED", {"file_path": "subagents/orchestrator.py"})

# Handlers automatically:
# 1. DependencyCheckHandler ‚Üí DEPENDENCY_IMPACT_DETECTED
# 2. ExtendedThinkingHandler ‚Üí REFACTORING_RECOMMENDATION (if critical)
# 3. ObservabilityDashboard ‚Üí Real-time visualization
```

## Workflows

### Safe Module Rename (Event-Driven)

**Automated workflow**:
1. Emit `FILE_MODIFIED` event (via hook or manual)
2. `DependencyCheckHandler` analyzes impact
3. If critical ‚Üí `ExtendedThinkingHandler` provides AI recommendations
4. Observability Dashboard shows real-time events
5. Follow recommended sequence from AI
6. Verify via test suite

**Manual workflow**:
```bash
# Step 1: Analyze
uv run scripts/analyze_impact.py subagents/orchestrator.py

# Step 2: Review output
# - Impact: 12 nodes
# - Critical: YES
# - Recommended sequence:
#   1. Create new file orch.py (copy)
#   2. Update subagents/__init__.py imports
#   3. Update main.py, generate_agent_rules.py
#   4. Run full test suite
#   5. Delete orchestrator.py after verification

# Step 3: Execute with lock
uv run scripts/safe_rename.py --old orchestrator.py --new orch.py

# Step 4: Verify
pytest tests/
```

### Batch Refactoring (Parallel)

**Use case**: Rename 20+ modules in parallel

```python
from containers import Container

container = Container()
lock_manager = container.refactoring_lock_manager()
event_router = container.event_router()

# Emit batch refactor request
await event_router.publish("BATCH_REFACTOR_REQUEST", {
    "files": ["subagents/agent1.py", "subagents/agent2.py", ...],
    "refactoring_type": "rename",
    "batch_id": "batch-001"
})

# BatchRefactoringHandler:
# 1. Acquires locks for all files
# 2. Emits FILE_MODIFIED events in parallel
# 3. Aggregates recommendations
# 4. Releases locks
# 5. Emits BATCH_REFACTOR_COMPLETE
```

## Integration Points

**EventRouter**:
- `FILE_MODIFIED` ‚Üí `DEPENDENCY_IMPACT_DETECTED`
- `CRITICAL_DEPENDENCY_ALERT` ‚Üí `REFACTORING_RECOMMENDATION`
- `BATCH_REFACTOR_REQUEST` ‚Üí `BATCH_REFACTOR_COMPLETE`

**Lock Manager**:
- Prevents concurrent modifications
- TTL-based cleanup (30s default)
- Heartbeat support for long operations

**Observability Dashboard**:
- Real-time event visualization
- Critical path highlighting
- Impact heatmap

**Hooks**:
- `pre_tool_use.py`: Auto-emits FILE_MODIFIED on Edit tool

## Reference

- [Event Integration](reference/event-integration.md): Event-driven patterns
- [Dependency Analysis](reference/dependency-analysis.md): DependencyAgent API
- [Lock Manager](reference/lock-manager.md): Concurrent refactoring guide
- [Safe Workflows](reference/safe-workflows.md): Step-by-step guides
```

### 4.3 CLI Script: analyze_impact.py

**Íµ¨ÌòÑ ÏúÑÏπò**: `.claude/skills/refactoring-safety/scripts/analyze_impact.py`

```python
"""
CLI wrapper for DependencyAgent impact analysis
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from lib.dependency_analyzer import DependencyAgent
import argparse

def main():
    parser = argparse.ArgumentParser(description="Analyze refactoring impact")
    parser.add_argument("file_path", help="File to analyze")
    parser.add_argument("--depth", type=int, default=2, help="Traversal depth")
    args = parser.parse_args()

    # Initialize agent
    agent = DependencyAgent()

    # Analyze impact
    impact_nodes = agent.get_impact_set([args.file_path], depth=args.depth)
    critical_affected = any(node.is_critical for node in impact_nodes)

    # Output
    print(f"\nüìä Impact Analysis: {args.file_path}")
    print(f"{'='*60}")
    print(f"Blast Radius: {len(impact_nodes)} nodes")
    print(f"Critical Path Affected: {'‚ö†Ô∏è  YES' if critical_affected else '‚úÖ NO'}")
    print(f"\nDirect Dependents (who imports this?):")
    dependents = agent.get_dependents(args.file_path, depth=1)
    for dep in dependents[:10]:
        print(f"  - {dep.node_id}")
    print(f"\nDependencies (what does this import?):")
    dependencies = agent.get_dependencies(args.file_path, depth=1)
    for dep in dependencies[:10]:
        print(f"  - {dep.node_id}")

    if critical_affected:
        print(f"\n‚ö†Ô∏è  CRITICAL: This change affects critical paths!")
        print(f"Recommendation: Run Extended Thinking analysis via event:")
        print(f"  await event_router.publish('FILE_MODIFIED', {{'file_path': '{args.file_path}'}})")

if __name__ == "__main__":
    main()
```

### 4.4 Deliverables

‚úÖ `.claude/skills/refactoring-safety/SKILL.md`
‚úÖ `.claude/skills/refactoring-safety/scripts/analyze_impact.py`
‚úÖ `.claude/skills/refactoring-safety/reference/*.md` (4 files)
‚úÖ `.claude/skills/refactoring-safety/resources/refactoring_templates/*.yaml` (3 templates)

---

## üéØ Phase 5: Observability Dashboard Integration

**ÏÜåÏöî ÏãúÍ∞Ñ**: 2Ïùº
**Î™©Ìëú**: Visualize dependency events in existing dashboard
**Ïö∞ÏÑ†ÏàúÏúÑ**: üü° Medium

### 5.1 Dashboard Component: DependencyEventViewer.vue

**Íµ¨ÌòÑ ÏúÑÏπò**: `observability-dashboard/src/components/DependencyEventViewer.vue`

```vue
<template>
  <div class="dependency-viewer">
    <h3>Dependency Impact Events</h3>

    <!-- Filters -->
    <div class="filters">
      <label>
        <input type="checkbox" v-model="showCriticalOnly" />
        Show Critical Only
      </label>
      <label>
        Batch:
        <select v-model="selectedBatch">
          <option value="">All</option>
          <option v-for="bid in batchIds" :key="bid" :value="bid">{{ bid }}</option>
        </select>
      </label>
    </div>

    <!-- Event List -->
    <div class="event-list">
      <div
        v-for="event in filteredEvents"
        :key="event.id"
        :class="['event-card', event.critical ? 'critical' : 'safe']"
      >
        <div class="event-header">
          <span class="badge" :class="event.critical ? 'critical' : 'safe'">
            {{ event.critical ? 'üî¥ CRITICAL' : 'üü¢ SAFE' }}
          </span>
          <span class="timestamp">{{ formatTimestamp(event.timestamp) }}</span>
        </div>

        <div class="event-body">
          <div class="file-path">{{ event.file_path }}</div>
          <div class="impact-stats">
            <span>Impact: {{ event.impact_size }} nodes</span>
            <span v-if="event.batch_id">Batch: {{ event.batch_id }}</span>
          </div>
        </div>

        <div v-if="event.recommendation" class="recommendation">
          <strong>AI Recommendation:</strong>
          <ul>
            <li v-for="(step, i) in event.recommendation.sequence" :key="i">
              {{ step }}
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</template>

<script>
export default {
  data() {
    return {
      showCriticalOnly: false,
      selectedBatch: ''
    };
  },
  computed: {
    dependencyEvents() {
      return this.$store.state.events.filter(e =>
        e.event_type === 'DEPENDENCY_IMPACT_DETECTED'
      );
    },
    batchIds() {
      return [...new Set(this.dependencyEvents.map(e => e.batch_id).filter(Boolean))];
    },
    filteredEvents() {
      let events = this.dependencyEvents;

      if (this.showCriticalOnly) {
        events = events.filter(e => e.critical_affected);
      }

      if (this.selectedBatch) {
        events = events.filter(e => e.batch_id === this.selectedBatch);
      }

      return events;
    }
  },
  methods: {
    formatTimestamp(ts) {
      return new Date(ts).toLocaleTimeString();
    }
  }
};
</script>

<style scoped>
.dependency-viewer {
  padding: 20px;
}

.event-card {
  border: 2px solid #ddd;
  border-radius: 8px;
  padding: 15px;
  margin-bottom: 10px;
}

.event-card.critical {
  border-color: #ff4444;
  background: #fff5f5;
}

.event-card.safe {
  border-color: #44ff44;
  background: #f5fff5;
}

.badge {
  padding: 4px 8px;
  border-radius: 4px;
  font-weight: bold;
}

.badge.critical {
  background: #ff4444;
  color: white;
}

.badge.safe {
  background: #44ff44;
  color: white;
}

.recommendation {
  margin-top: 10px;
  background: #f0f8ff;
  padding: 10px;
  border-radius: 4px;
}
</style>
```

### 5.2 App.vue Integration

```vue
<template>
  <div class="dashboard">
    <!-- Existing components -->
    <EventTimeline />
    <WorkflowProgress />

    <!-- NEW -->
    <DependencyEventViewer />
  </div>
</template>

<script>
import DependencyEventViewer from './components/DependencyEventViewer.vue';

export default {
  components: {
    // ... existing ...
    DependencyEventViewer
  }
};
</script>
```

### 5.3 Event Aggregation (Gap Analysis Issue 5)

**Íµ¨ÌòÑ ÏúÑÏπò**: `observability-server/server.py` (ÌôïÏû•)

```python
# Add rate limiting and aggregation
from collections import defaultdict
from datetime import datetime, timedelta

class EventAggregator:
    """Aggregate events to prevent dashboard overload"""

    def __init__(self, window_seconds=5, max_events_per_window=10):
        self.window_seconds = window_seconds
        self.max_events = max_events_per_window
        self.event_buffer = defaultdict(list)

    def add_event(self, event: dict) -> bool:
        """
        Add event to buffer, returns True if should emit immediately
        """
        batch_id = event.get("batch_id", "default")
        self.event_buffer[batch_id].append(event)

        # Check if buffer is full
        if len(self.event_buffer[batch_id]) >= self.max_events:
            self._flush_batch(batch_id)
            return False  # Emit aggregated event instead

        return True  # Emit individual event

    def _flush_batch(self, batch_id: str):
        """Emit aggregated event for batch"""
        events = self.event_buffer[batch_id]
        aggregated = {
            "event_type": "DEPENDENCY_BATCH_AGGREGATED",
            "batch_id": batch_id,
            "event_count": len(events),
            "critical_count": sum(1 for e in events if e.get("critical_affected")),
            "total_impact": sum(e.get("impact_size", 0) for e in events),
            "timestamp": datetime.now().isoformat()
        }

        # Send to WebSocket
        emit_to_websocket(aggregated)

        # Clear buffer
        self.event_buffer[batch_id] = []
```

### 5.4 Deliverables

‚úÖ `observability-dashboard/src/components/DependencyEventViewer.vue`
‚úÖ `observability-dashboard/src/App.vue` (integration)
‚úÖ `observability-server/server.py` (EventAggregator)
‚úÖ E2E test: `observability-dashboard/tests/test_dependency_viewer.spec.ts`

---

## üéØ Phase 6: Batch Refactoring Handler

**ÏÜåÏöî ÏãúÍ∞Ñ**: 2Ïùº
**Î™©Ìëú**: Parallel event-driven refactoring with lock coordination
**Ïö∞ÏÑ†ÏàúÏúÑ**: üîµ Medium

### 6.1 BatchRefactoringHandler Íµ¨ÌòÑ

**Íµ¨ÌòÑ ÏúÑÏπò**: `handlers/batch_refactoring_handler.py`

```python
"""
Batch Refactoring Handler - Parallel event-driven refactoring
Uses RefactoringLockManager to prevent concurrent modifications
"""
from lib.event_router import EventRouter
from lib.refactoring_lock_manager import RefactoringLockManager
from typing import Dict, Any
import asyncio

class BatchRefactoringHandler:
    """
    Event flow:
    BATCH_REFACTOR_REQUEST ‚Üí acquire locks ‚Üí emit FILE_MODIFIED (parallel) ‚Üí BATCH_REFACTOR_COMPLETE
    """

    def __init__(self, event_router: EventRouter, lock_manager: RefactoringLockManager):
        self.event_router = event_router
        self.lock_manager = lock_manager

        event_router.subscribe("BATCH_REFACTOR_REQUEST", self.handle_batch)

    async def handle_batch(self, data: Dict[str, Any]):
        """Process batch refactoring in parallel with lock coordination"""
        files = data["files"]  # List of file paths
        batch_id = data.get("batch_id", f"batch-{asyncio.current_task().get_name()}")
        refactoring_type = data.get("refactoring_type", "unknown")

        # 1. Acquire locks for all files
        tokens = []
        try:
            for file in files:
                token = await self.lock_manager.acquire_lock(file, owner=batch_id)
                tokens.append(token)
        except Exception as e:
            # Release acquired locks on failure
            for token in tokens:
                self.lock_manager.release_lock(token)
            raise

        try:
            # 2. Emit FILE_MODIFIED events in parallel
            tasks = [
                self.event_router.publish("FILE_MODIFIED", {
                    "file_path": f,
                    "batch_id": batch_id
                })
                for f in files
            ]

            await asyncio.gather(*tasks)

            # 3. Wait for all recommendations (simplified - use proper coordination)
            await asyncio.sleep(2)

            # 4. Publish completion
            await self.event_router.publish("BATCH_REFACTOR_COMPLETE", {
                "batch_id": batch_id,
                "files_processed": len(files),
                "refactoring_type": refactoring_type
            })

        finally:
            # 5. Release all locks
            for token in tokens:
                self.lock_manager.release_lock(token)
```

### 6.2 Container Integration

```python
# containers.py
batch_refactoring_handler = providers.Singleton(
    BatchRefactoringHandler,
    event_router=event_router,
    lock_manager=refactoring_lock_manager
)
```

### 6.3 Usage Example

```python
# Refactor 20 modules in parallel
from containers import Container

container = Container()
event_router = container.event_router()

await event_router.publish("BATCH_REFACTOR_REQUEST", {
    "files": [f"subagents/agent{i}.py" for i in range(1, 21)],
    "refactoring_type": "rename",
    "batch_id": "rename-agents-batch"
})
```

### 6.4 Deliverables

‚úÖ `handlers/batch_refactoring_handler.py`
‚úÖ `containers.py` (Handler registration)
‚úÖ Unit tests: `tests/test_batch_refactoring_handler.py`

---

## üéØ Phase 7: File Watcher Daemon (Optional)

**ÏÜåÏöî ÏãúÍ∞Ñ**: 2Ïùº
**Î™©Ìëú**: Continuous monitoring with file system events
**Ïö∞ÏÑ†ÏàúÏúÑ**: üîµ Low (Nice-to-have)

### 7.1 Dependency Watch Daemon

**Íµ¨ÌòÑ ÏúÑÏπò**: `tools/dependency_watch_daemon.py`

```python
"""
Dependency Watch Daemon - File system monitoring
Emits FILE_MODIFIED events on file changes
"""
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from containers import Container
import asyncio
import time

class DependencyWatchHandler(FileSystemEventHandler):
    """Emit FILE_MODIFIED events on file changes"""

    def __init__(self, event_router):
        self.event_router = event_router
        self.debounce_cache = {}  # file_path -> last_modified_time
        self.debounce_seconds = 1  # Ignore changes within 1 second (Gap Analysis Issue 5)

    def on_modified(self, event):
        """Called when a file is modified"""
        if event.is_directory:
            return

        if not event.src_path.endswith('.py'):
            return

        # Debounce: ignore rapid successive changes
        now = time.time()
        last_modified = self.debounce_cache.get(event.src_path, 0)
        if now - last_modified < self.debounce_seconds:
            return

        self.debounce_cache[event.src_path] = now

        # Emit event
        asyncio.create_task(self.event_router.publish("FILE_MODIFIED", {
            "file_path": event.src_path,
            "source": "file_watcher"
        }))

def start_daemon():
    """Start file watcher daemon"""
    container = Container()
    event_router = container.event_router()

    handler = DependencyWatchHandler(event_router)
    observer = Observer()
    observer.schedule(handler, "/home/kc-palantir/math", recursive=True)
    observer.start()

    print("üîÑ Dependency watch daemon started")
    print("   Monitoring: /home/kc-palantir/math")
    print("   Debounce: 1 second")
    print("   Press Ctrl+C to stop")

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
        print("\n‚úÖ Daemon stopped")

    observer.join()

if __name__ == "__main__":
    start_daemon()
```

### 7.2 Usage

```bash
# Start daemon in background
uv run tools/dependency_watch_daemon.py &

# Daemon will automatically:
# 1. Watch for .py file changes
# 2. Emit FILE_MODIFIED events
# 3. Trigger dependency analysis via handlers
# 4. Send events to observability dashboard
```

### 7.3 Deliverables

‚úÖ `tools/dependency_watch_daemon.py`
‚úÖ Integration test: `tests/test_file_watcher_integration.py`

---

## üìã Íµ¨ÌòÑ Ïö∞ÏÑ†ÏàúÏúÑ Î∞è ÌÉÄÏûÑÎùºÏù∏ (Senior Review Î∞òÏòÅ)

### üî• Week 1: MVP Sprint (5Ïùº)

**Priority 1: Í∏∞Î∞ò Íµ¨Ï∂ï + ÏïàÏ†Ñ Ïû•Ïπò**

1. ‚úÖ **Phase 0**: Baseline Metrics (2Ïùº, +1Ïùº)
   - `scripts/generate_dependency_report.py` (mode Ï∂îÍ∞Ä)
   - `.claude/config/critical_paths.json` + `.local.json`
   - `reports/DEPENDENCY_BASELINE_REPORT.md` + `COMPARISON.md`
   - Git hook integration (post-commit)
   - **ÏòàÏÉÅ Ìö®Í≥º**: Î¶¨Ìå©ÌÜ†ÎßÅ Ìö®Í≥º Ï∏°Ï†ï Í∏∞Ï§ÄÏÑ† ÌôïÎ≥¥

2. ‚úÖ **Phase 1**: DependencyCheckHandler (2Ïùº)
   - `handlers/dependency_check_handler.py`
   - `containers.py` ÌÜµÌï©
   - `.claude/hooks/pre_tool_use.py` ÌôïÏû• (blocking logic)
   - **ÏòàÏÉÅ Ìö®Í≥º**: ÌååÏùº ÏàòÏ†ï Ïãú ÏûêÎèô impact Î∂ÑÏÑù + Ï∞®Îã®

3. üîß **Phase 3**: RefactoringLockManager (1Ïùº, Ïû¨Î∞∞Ïπò)
   - `lib/refactoring_lock_manager.py` (multi-backend)
   - File-based lock Íµ¨ÌòÑ
   - **ÏòàÏÉÅ Ìö®Í≥º**: Í≤ΩÏüÅ Ï°∞Í±¥ Î∞©ÏßÄ (Î∂ÑÏÇ∞ ÌôòÍ≤Ω ÏßÄÏõê)

**Deliverable**: Í∏∞Î≥∏ event-driven analysis + safety mechanisms

### üü° Week 2: Intelligence Sprint (5Ïùº)

**Priority 2: AI & Feature Flags**

4. üîß **Phase 2**: ExtendedThinkingHandler (3Ïùº, +1Ïùº)
   - `handlers/extended_thinking_handler.py`
   - TTL caching + fallback logic
   - Cost tracking & metrics
   - **ÏòàÏÉÅ Ìö®Í≥º**: AI-assisted refactoring planning (cost-optimized)

5. üìù **Infrastructure**: Feature Flags & Config (2Ïùº)
   - `lib/feature_flags.py` Íµ¨ÌòÑ
   - Performance SLOs Ï†ïÏùò (`.claude/config/performance_slos.yaml`)
   - User notification strategy
   - **ÏòàÏÉÅ Ìö®Í≥º**: Production-ready configuration

**Deliverable**: AI-assisted workflows with cost control

### üîµ Week 3-4: Visibility & Workflow Sprint (7Ïùº)

**Priority 3: ÏõåÌÅ¨ÌîåÎ°úÏö∞ & Í∞ÄÏãúÏÑ±**

6. ‚öôÔ∏è **Phase 4**: Refactoring Safety Skill (3Ïùº)
   - `.claude/skills/refactoring-safety/`
   - CLI wrappers (analyze_impact.py, rollback.py)
   - Dry-run mode Íµ¨ÌòÑ
   - Event integration docs
   - **ÏòàÏÉÅ Ìö®Í≥º**: Workflow reusability + rollback ÏßÄÏõê

7. ‚öôÔ∏è **Phase 5**: Observability Dashboard (3Ïùº, +1Ïùº)
   - `DependencyEventViewer.vue`
   - Event aggregation (Gap Analysis Issue 5)
   - Batch grouping & filters
   - **ÏòàÏÉÅ Ìö®Í≥º**: Real-time visibility

8. üß™ **Testing**: Integration & E2E (1Ïùº)
   - Integration tests (event flow)
   - E2E workflow tests (5 scenarios)
   - **ÏòàÏÉÅ Ìö®Í≥º**: Quality assurance

**Deliverable**: Production-ready system with full observability

### üü¢ Week 5: Scale & Polish Sprint (2Ïùº)

**Priority 4: ÌôïÏû• Í∏∞Îä•**

9. üöÄ **Phase 6**: BatchRefactoringHandler (2Ïùº)
   - `handlers/batch_refactoring_handler.py`
   - Lock coordination
   - Parallel event emission
   - **ÏòàÏÉÅ Ìö®Í≥º**: ÎåÄÍ∑úÎ™® Î≥ëÎ†¨ Î¶¨Ìå©ÌÜ†ÎßÅ

**Deliverable**: Scale to 20+ files in parallel

### üöÄ Week 6: Optional Sprint (2Ïùº)

**Priority 5: ÏßÄÏÜçÏ†Å Î™®ÎãàÌÑ∞ÎßÅ (Optional)**

10. üéØ **Phase 7**: File Watcher Daemon (2Ïùº)
    - `tools/dependency_watch_daemon.py`
    - Debounce logic
    - Resource leak prevention
    - **ÏòàÏÉÅ Ìö®Í≥º**: ÏßÄÏÜçÏ†Å Î™®ÎãàÌÑ∞ÎßÅ

**Total**: 
- **Core (Phase 0-6)**: 19Ïùº (3Ï£º + 4Ïùº)
- **With Optional (Phase 7)**: 21Ïùº (4Ï£º + 1Ïùº)

**ÌÉÄÏûÑÎùºÏù∏ Ï°∞Ï†ï Í∑ºÍ±∞** (Senior Review):
- Phase 0: +1Ïùº (comparison mode, git hooks)
- Phase 2: +1Ïùº (caching, fallback logic)
- Phase 3: +1Ïùº (file-based backend)
- Phase 5: +1Ïùº (aggregation, batch grouping)
- Infrastructure: +2Ïùº (feature flags, SLOs)
- **Ï¥ù Î≤ÑÌçº**: +27% (15Ïùº ‚Üí 19Ïùº)

---

## üìä ÏòàÏÉÅ Ìö®Í≥º

### Before (ÌòÑÏû¨ ÏÉÅÌÉú)

| ÏßÄÌëú | ÌòÑÏû¨ |
|------|------|
| ‚è±Ô∏è ÏùòÏ°¥ÏÑ± Î∂ÑÏÑù | ÏàòÎèô (DependencyAgent CLI Ìò∏Ï∂ú) |
| üìä Impact Í∞ÄÏãúÏÑ± | CLI Ï∂úÎ†•Îßå |
| ü§ñ ÏûêÎèôÌôî | ÏóÜÏùå |
| üîÑ Monitoring | ÏàòÎèô |
| ‚ö†Ô∏è Breaking change Î∞©ÏßÄ | ÏóÜÏùå |
| üîí Í≤ΩÏüÅ Ï°∞Í±¥ Î∞©ÏßÄ | ÏóÜÏùå |

### After (Phase 0-6 ÏôÑÎ£å ÌõÑ)

| ÏßÄÌëú | Í∞úÏÑ† ÌõÑ | Î∞©Î≤ï |
|------|---------|------|
| ‚è±Ô∏è ÏùòÏ°¥ÏÑ± Î∂ÑÏÑù | **ÏûêÎèô** (0Ï¥à) | Event-driven handler |
| üìä Impact Í∞ÄÏãúÏÑ± | **Ïã§ÏãúÍ∞Ñ Dashboard** | Observability integration |
| ü§ñ ÏûêÎèôÌôî | **90%** | Hook + EventRouter |
| üîÑ Monitoring | **ÏßÄÏÜçÏ†Å** | File watcher daemon |
| ‚ö†Ô∏è Breaking change Î∞©ÏßÄ | **90%+** | Pre-commit hook + Critical path alert |
| üîí Í≤ΩÏüÅ Ï°∞Í±¥ Î∞©ÏßÄ | **100%** | RefactoringLockManager (TTL + heartbeat) |

### Í∞úÏÑ†Ïú®

- ‚è±Ô∏è **ÏãúÍ∞Ñ**: ÏàòÎèô 5Î∂Ñ ‚Üí ÏûêÎèô 0Ï¥à (**100% Îã®Ï∂ï**)
- ‚ö†Ô∏è **ÏóêÎü¨**: ~15% ‚Üí <2% (**87% Í∞êÏÜå**)
- ü§ñ **ÏûêÎèôÌôî**: 0% ‚Üí 90% (**90% Ìñ•ÏÉÅ**)
- üîç **Í∞ÄÏãúÏÑ±**: CLIÎßå ‚Üí Dashboard + AI recommendations

---

## üõ°Ô∏è Î¶¨Ïä§ÌÅ¨ ÎåÄÏùë (Gap Analysis ÌÜµÌï©)

### Issue 1: Event Ordering & Race Conditions ‚úÖ Ìï¥Í≤∞
**ÎåÄÏùë**: Phase 3 RefactoringLockManager
- TTL-based lock (30Ï¥à)
- Heartbeat for long operations
- ÏßÄÏàò Î∞±Ïò§ÌîÑ Ïû¨ÏãúÎèÑ

### Issue 2: Cache Invalidation ‚úÖ Ìï¥Í≤∞
**ÎåÄÏùë**: Phase 1.1 DependencyCheckHandler
- `_is_working_dir_clean()` Ï≤¥ÌÅ¨
- Uncommitted Î≥ÄÍ≤Ω Í∞êÏßÄ Ïãú Ï∫êÏãú Î¨¥Ïãú

### Issue 3: Hook Execution Overhead ‚úÖ Ìï¥Í≤∞
**ÎåÄÏùë**: Phase 1.1 DependencyCheckHandler
- Lazy initialization
- Singleton pattern (agent Ïû¨ÏÇ¨Ïö©)
- Background warmup

### Issue 4: Container Initialization Complexity ‚úÖ Ìï¥Í≤∞
**ÎåÄÏùë**: Phase 1.1 DependencyCheckHandler
- Lazy handler initialization
- Cache-first strategy

### Issue 5: Event Storm on Batch Operations ‚úÖ Ìï¥Í≤∞
**ÎåÄÏùë**: Phase 5.3 Event Aggregation
- Rate limiting (Ï¥àÎãπ 10Í∞ú)
- Batch aggregation (5Ï¥à window)
- DashboardÏóêÏÑú batch Îã®ÏúÑ ÌëúÏãú

### Issue 6: Missing Critical Path Definition ‚úÖ Ìï¥Í≤∞
**ÎåÄÏùë**: Phase 0.2 Critical Path JSON
- `.claude/config/critical_paths.json`
- Pattern matching ÏßÄÏõê
- DependencyAgentÏóêÏÑú JSON Î°úÎìú

### Issue 7: Circular Dependency ÎØ∏Í∞êÏßÄ ‚úÖ Ìï¥Í≤∞
**ÎåÄÏùë**: Phase 0.1 Baseline Metrics
- `networkx.simple_cycles()` ÏÇ¨Ïö©
- Baseline reportÏóê ÏàúÌôò ÏùòÏ°¥ÏÑ± Î™©Î°ù
- HookÏóêÏÑú Ïã†Í∑ú ÏàúÌôò Í∞êÏßÄ Ïãú CRITICAL alert

---

## ‚úÖ Í≤ÄÏ¶ù Í≥ÑÌöç

### Unit Tests

```bash
# Phase 0
pytest tests/test_generate_dependency_report.py

# Phase 1
pytest tests/test_dependency_check_handler.py

# Phase 2
pytest tests/test_extended_thinking_handler.py

# Phase 3
pytest tests/test_refactoring_lock_manager.py

# Phase 4
pytest tests/test_refactoring_safety_skill.py

# Phase 5
pytest tests/test_dependency_event_viewer.py

# Phase 6
pytest tests/test_batch_refactoring_handler.py
```

### Integration Tests

```python
# tests/test_event_driven_refactoring.py
async def test_file_modified_event_flow():
    """End-to-end event flow test"""
    container = Container()
    event_router = container.event_router()

    # Track published events
    impact_events = []
    recommendation_events = []

    async def capture_impact(data):
        impact_events.append(data)

    async def capture_recommendation(data):
        recommendation_events.append(data)

    event_router.subscribe("DEPENDENCY_IMPACT_DETECTED", capture_impact)
    event_router.subscribe("REFACTORING_RECOMMENDATION", capture_recommendation)

    # Emit FILE_MODIFIED for critical file
    await event_router.publish("FILE_MODIFIED", {
        "file_path": "main.py"  # Critical path
    })

    # Wait for async processing
    await asyncio.sleep(0.5)

    # Assert
    assert len(impact_events) == 1
    assert impact_events[0]["critical_affected"] == True
    assert len(recommendation_events) == 1  # Extended Thinking triggered
```

### E2E Workflow Test

```bash
# tests/test_refactoring_workflow_e2e.py
pytest tests/test_refactoring_workflow_e2e.py -v

# Scenarios:
# 1. Safe change (non-critical file)
# 2. Critical change (triggers AI recommendation)
# 3. Batch refactoring (20 files in parallel)
# 4. Lock contention (concurrent modifications)
# 5. Circular dependency detection
```

### Chaos Engineering Test (Gap Analysis Î≥¥ÏôÑ)

```bash
# tests/test_chaos_dependencies.py
pytest tests/test_chaos_dependencies.py -v

# Scenarios:
# 1. Random import addition/deletion
# 2. Circular dependency creation
# 3. Critical file deletion attempt (should block)
# 4. System stability under random changes
```

### Performance Benchmarks

```bash
# tests/benchmark_parallel_refactor.py
uv run tests/benchmark_parallel_refactor.py

# Metrics:
# - Sequential vs Parallel (speedup %)
# - Lock overhead (ms)
# - Event aggregation efficiency
# - Dashboard response time
```

---

## üìö Í∏∞Ïà† Ïä§ÌÉù Î∞è Ï∞∏Í≥† ÏûêÎ£å

### ÌïµÏã¨ Í∏∞Ïà†

| Íµ¨ÏÑ±ÏöîÏÜå | Í∏∞Ïà† | Ïö©ÎèÑ |
|---------|------|------|
| Dependency Analysis | NetworkX + AST | Í∑∏ÎûòÌîÑ Íµ¨Ï∂ï Î∞è traversal |
| Event-Driven | EventRouter (pub/sub) | ÎπÑÎèôÍ∏∞ event handling |
| DI Container | dependency-injector | Handler lifecycle Í¥ÄÎ¶¨ |
| Observability | Bun + SQLite + Vue.js | Real-time monitoring |
| Lock Manager | asyncio.Lock + TTL | Í≤ΩÏüÅ Ï°∞Í±¥ Î∞©ÏßÄ |
| Extended Thinking | Claude Sonnet 4.5 | AI-assisted planning |

### Ïô∏Î∂Ä ÏùòÏ°¥ÏÑ±

```toml
[tool.uv.dependencies]
# Í∏∞Ï°¥ (Ïù¥ÎØ∏ ÏÑ§ÏπòÎê®)
networkx = "^3.0"
dependency-injector = "^4.0"

# Ïã†Í∑ú Ï∂îÍ∞Ä
watchdog = "^3.0"  # File watcher (Phase 7)
pyvis = "^0.3"     # Graph visualization (Phase 0)
```

### Claude API Features

| Feature | Beta Header | Usage |
|---------|-------------|-------|
| Extended Thinking | `interleaved-thinking-2025-05-14` | Phase 2 |
| Memory Tool | `memory-tool-2025-10-02` | Optional (pickle Ï∂©Î∂Ñ) |

### Ï∞∏Í≥† Î¨∏ÏÑú

**Event-Driven Patterns**:
- Martin Fowler: Event-Driven Architecture
- Reactive Manifesto: https://www.reactivemanifesto.org

**Existing Codebase**:
- `lib/event_router.py`: EventRouter implementation
- `lib/dependency_analyzer.py`: AST-based analysis (535 lines)
- `containers.py`: DI container patterns

**Dependency Injection**:
- dependency-injector docs: https://python-dependency-injector.ets-labs.org

**Claude Sonnet 4.5**:
- Extended Thinking: https://docs.claude.com/en/docs/build-with-claude/extended-thinking
- Agent Skills: https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview

---

## üîÑ v2.0 vs v3.0 vs ÏµúÏ¢Ö Í≥ÑÌöç ÎπÑÍµê

### ÏïÑÌÇ§ÌÖçÏ≤ò Í≤∞Ï†ï

| Ìï≠Î™© | v2.0 | v3.0 | ÏµúÏ¢Ö Í≥ÑÌöç |
|------|------|------|-----------|
| Agent ÌÜµÌï© | ClaudeSDKClient | EventRouter | EventRouter ‚úÖ |
| Ïã§Ìñâ Î™®Îç∏ | ÎèôÍ∏∞Ï†Å | ÎπÑÎèôÍ∏∞ event | ÎπÑÎèôÍ∏∞ event ‚úÖ |
| Import chain | main ‚Üí subagents | Container ‚Üí handlers | Container ‚Üí handlers ‚úÖ |
| Lock Manager | ‚ùå ÏóÜÏùå | ‚ùå ÏóÜÏùå | ‚úÖ Phase 3 Ï∂îÍ∞Ä |
| Baseline Metrics | ‚úÖ Phase 0 | ‚ùå ÏóÜÏùå | ‚úÖ Phase 0 (v2 Ï∞®Ïö©) |
| Event Aggregation | ‚ùå ÏóÜÏùå | ‚ùå ÏóÜÏùå | ‚úÖ Phase 5.3 Ï∂îÍ∞Ä |
| Cache Invalidation | Git commitÎßå | Git commitÎßå | ‚úÖ Working dir Í∞êÏßÄ |
| Circular Deps | ‚ùå ÎØ∏Í∞êÏßÄ | ‚ùå ÎØ∏Í∞êÏßÄ | ‚úÖ Phase 0 Í∞êÏßÄ |

### Phase ÎπÑÍµê

| Phase | v2.0 | v3.0 | ÏµúÏ¢Ö Í≥ÑÌöç |
|-------|------|------|-----------|
| 0 | Baseline Metrics | - | ‚úÖ Baseline + Critical Path JSON |
| 1 | Extended Thinking | DependencyCheckHandler | ‚úÖ DependencyCheckHandler (v3) |
| 2 | Memory Tool | ExtendedThinkingHandler | ‚úÖ ExtendedThinkingHandler (v3) |
| 3 | Refactoring Skill | Session State | ‚úÖ **Lock Manager (Gap Î≥¥ÏôÑ)** |
| 4 | Hook Integration | Refactoring Skill | ‚úÖ Refactoring Skill (v3) |
| 5 | Parallel Refactoring | Dashboard | ‚úÖ Dashboard + Aggregation (v3 + Gap) |
| 6 | Infinite Loop | BatchRefactoring | ‚úÖ BatchRefactoring (v3) |
| 7 | - | File Watcher | ‚úÖ File Watcher (v3, Optional) |

### Íµ¨ÌòÑ ÎÖ∏Î†•

| Íµ¨Î∂Ñ | v2.0 ÏòàÏÉÅ | v3.0 ÏòàÏÉÅ | ÏµúÏ¢Ö Í≥ÑÌöç Ïã§Ï†ú |
|------|-----------|-----------|----------------|
| Phase 0 | 1Ïùº | - | 1Ïùº |
| Phase 1 | 1Ïùº | 2Ïùº | 2Ïùº |
| Phase 2 | 1Ïùº | 2Ïùº | 2Ïùº |
| Phase 3 | 2Ïùº | 1Ïùº | 1Ïùº (Lock Manager) |
| Phase 4 | 2Ïùº | 3Ïùº | 3Ïùº |
| Phase 5 | 2Ïùº | 2Ïùº | 2Ïùº |
| Phase 6 | 2Ïùº | 2Ïùº | 2Ïùº |
| Phase 7 | 2Ïùº | 2Ïùº | 2Ïùº (Optional) |
| **Total** | **12Ïùº** | **14Ïùº** | **15Ïùº (Phase 7 Ï†úÏô∏ 13Ïùº)** |

---

## üéõÔ∏è Feature Flags & Configuration

### Feature Flags

**Íµ¨ÌòÑ ÏúÑÏπò**: `lib/feature_flags.py`

```python
"""
Feature flags for dependency refactoring system
Enables gradual rollout and A/B testing
"""
import os
from typing import Dict, Any

class FeatureFlags:
    """
    Feature flags loaded from environment variables
    Defaults are production-safe
    """
    
    # Extended Thinking
    ENABLE_EXTENDED_THINKING = os.getenv("FF_EXTENDED_THINKING", "true").lower() == "true"
    
    # Auto-blocking on critical paths
    ENABLE_AUTO_BLOCKING = os.getenv("FF_AUTO_BLOCKING", "true").lower() == "true"
    
    # File watcher daemon
    ENABLE_FILE_WATCHER = os.getenv("FF_FILE_WATCHER", "false").lower() == "true"
    
    # Lock backend selection
    LOCK_BACKEND = os.getenv("LOCK_BACKEND", "file")  # "memory" or "file"
    LOCK_TTL_SECONDS = int(os.getenv("LOCK_TTL_SECONDS", "30"))
    
    # Dashboard integration
    ENABLE_DASHBOARD_EVENTS = os.getenv("FF_DASHBOARD_EVENTS", "true").lower() == "true"
    
    # Dry-run mode (no actual modifications)
    DRY_RUN_MODE = os.getenv("DRY_RUN_MODE", "false").lower() == "true"
    
    @classmethod
    def to_dict(cls) -> Dict[str, Any]:
        """Export all flags as dictionary"""
        return {
            "extended_thinking": cls.ENABLE_EXTENDED_THINKING,
            "auto_blocking": cls.ENABLE_AUTO_BLOCKING,
            "file_watcher": cls.ENABLE_FILE_WATCHER,
            "lock_backend": cls.LOCK_BACKEND,
            "lock_ttl": cls.LOCK_TTL_SECONDS,
            "dashboard_events": cls.ENABLE_DASHBOARD_EVENTS,
            "dry_run": cls.DRY_RUN_MODE
        }
    
    @classmethod
    def print_status(cls):
        """Print current feature flag status"""
        print("üéõÔ∏è  Feature Flags Status:")
        for key, value in cls.to_dict().items():
            status = "‚úÖ ENABLED" if value == True or value != "memory" else "‚ùå DISABLED"
            print(f"  {key}: {value} ({status})")
```

**Environment Configurations**:

```bash
# .env.development
FF_EXTENDED_THINKING=false  # Save costs
FF_AUTO_BLOCKING=false  # Allow quick iteration
LOCK_BACKEND=memory  # Single process
DRY_RUN_MODE=true  # Safe testing

# .env.ci
FF_EXTENDED_THINKING=false  # CI doesn't need AI
FF_AUTO_BLOCKING=true  # Enforce safety
LOCK_BACKEND=file  # Multi-process support

# .env.production
FF_EXTENDED_THINKING=true  # Full AI assistance
FF_AUTO_BLOCKING=true  # Maximum safety
LOCK_BACKEND=file  # Distributed environment
FF_DASHBOARD_EVENTS=true  # Full observability
```

---

## üìä Performance SLOs

**Íµ¨ÌòÑ ÏúÑÏπò**: `.claude/config/performance_slos.yaml`

```yaml
# Performance Service Level Objectives
# Used for monitoring and alerting

dependency_analysis:
  max_latency_ms: 100
  description: "Local DependencyAgent analysis should complete within 100ms"
  percentile: p95
  
  max_memory_mb: 500
  description: "Graph caching should not exceed 500MB memory"
  
  cache_hit_rate_min: 0.8
  description: "At least 80% cache hit rate for repeated analyses"

extended_thinking:
  max_latency_sec: 15
  description: "Claude API call should timeout after 15 seconds"
  percentile: p99
  
  cache_hit_rate_min: 0.8
  description: "TTL cache should achieve 80% hit rate (5Î∂Ñ window)"
  
  cost_per_session_max_usd: 1.0
  description: "Single dev session should not exceed $1 in API calls"

hook_execution:
  max_latency_ms: 50
  description: "Pre-tool-use hook should complete within 50ms (non-blocking path)"
  percentile: p95
  
  timeout_ms: 2000
  description: "Hook timeout for critical path analysis"

event_aggregation:
  max_events_per_window: 10
  description: "Max events per 5-second aggregation window"
  
  max_batch_size: 20
  description: "Max files in single batch refactoring operation"

lock_manager:
  default_ttl_seconds: 30
  description: "Default lock TTL before force-release"
  
  max_contention_wait_seconds: 60
  description: "Max time to wait for lock acquisition"

dashboard:
  max_websocket_latency_ms: 200
  description: "Real-time event delivery latency"
  percentile: p95
```

**Monitoring Integration**:
```python
# infrastructure/slo_monitor.py
class SLOMonitor:
    """Monitor and alert on SLO violations"""
    
    def check_dependency_analysis_latency(self, latency_ms: float):
        if latency_ms > 100:
            self.alert("dependency_analysis_latency_violation", latency_ms)
    
    def check_extended_thinking_cost(self, session_cost_usd: float):
        if session_cost_usd > 1.0:
            self.alert("extended_thinking_cost_violation", session_cost_usd)
```

---

## üîÑ Rollback Plan

### Automated Rollback

**Íµ¨ÌòÑ ÏúÑÏπò**: `scripts/rollback_refactoring.py`

```python
"""
Rollback refactoring changes safely
"""
import argparse
from lib.dependency_analyzer import DependencyAgent
import subprocess
import json

def rollback_batch(batch_id: str, dry_run: bool = False):
    """
    Rollback batch refactoring by batch_id
    
    Steps:
    1. Load batch metadata from observability DB
    2. Identify all modified files
    3. Git revert changes
    4. Rebuild dependency graph
    5. Verify no critical path breakage
    """
    
    # 1. Load batch metadata
    metadata = load_batch_metadata(batch_id)
    files = metadata["files"]
    commit_hash = metadata["commit_hash"]
    
    print(f"üìã Rollback Plan:")
    print(f"  Batch ID: {batch_id}")
    print(f"  Files: {len(files)}")
    print(f"  Commit: {commit_hash}")
    
    if dry_run:
        print("\nüü° DRY RUN - No changes will be made")
        return
    
    # 2. Create rollback branch
    branch_name = f"rollback/{batch_id}"
    subprocess.run(["git", "checkout", "-b", branch_name])
    
    # 3. Revert changes
    for file in files:
        print(f"  ‚è™ Reverting: {file}")
        subprocess.run(["git", "checkout", commit_hash, "--", file])
    
    # 4. Rebuild dependency graph
    agent = DependencyAgent()
    graph = agent.build_and_cache_graph(force=True)
    
    # 5. Verify no breakage
    import_errors = agent.check_import_errors()
    if import_errors:
        print(f"\n‚ö†Ô∏è  Import errors detected after rollback:")
        for error in import_errors:
            print(f"  - {error}")
        print("\n‚ùå Rollback may have introduced issues")
        return False
    
    print(f"\n‚úÖ Rollback successful")
    print(f"   Review changes in branch: {branch_name}")
    print(f"   To apply: git checkout main && git merge {branch_name}")
    
    return True

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch-id", required=True)
    parser.add_argument("--dry-run", action="store_true")
    args = parser.parse_args()
    
    rollback_batch(args.batch_id, args.dry_run)
```

**Usage**:
```bash
# Rollback specific batch
uv run scripts/rollback_refactoring.py --batch-id batch-001 --dry-run

# Apply rollback
uv run scripts/rollback_refactoring.py --batch-id batch-001
```

### Manual Rollback Workflow

```markdown
# Manual Rollback Guide

## Scenario 1: Recent Batch Refactoring Failed

1. Identify batch ID from observability dashboard
2. Run rollback script:
   ```bash
   uv run scripts/rollback_refactoring.py --batch-id <batch-id>
   ```
3. Review changes in rollback branch
4. Merge if verified

## Scenario 2: Gradual Rollback (Partial)

1. List files in batch:
   ```bash
   uv run scripts/list_batch_files.py --batch-id <batch-id>
   ```
2. Manually revert specific files:
   ```bash
   git checkout <commit> -- path/to/file.py
   ```
3. Test incrementally

## Scenario 3: Full System Rollback

1. Tag current state:
   ```bash
   git tag -a "pre-rollback-$(date +%s)" -m "Before full rollback"
   ```
2. Revert to baseline commit:
   ```bash
   git revert <baseline-commit>
   ```
3. Rebuild dependency graph:
   ```bash
   uv run scripts/generate_dependency_report.py --mode=snapshot
   ```
```

---

## üìù Implementation Checklist

### Phase 0: Baseline Metrics
- [ ] Create `scripts/generate_dependency_report.py` (with compare mode)
- [ ] Create `.claude/config/critical_paths.json` + `.local.json`
- [ ] Modify `lib/dependency_analyzer.py` (JSON loading)
- [ ] Generate initial baseline report
- [ ] Setup git hooks (post-commit)
- [ ] Commit baseline to git

### Phase 1: Event Integration
- [ ] Create `handlers/dependency_check_handler.py` (with blocking)
- [ ] Add handler to `containers.py`
- [ ] Extend `.claude/hooks/pre_tool_use.py` (critical path blocking)
- [ ] Test event flow end-to-end
- [ ] Add cache invalidation logic (git status check)

### Phase 2: Extended Thinking
- [ ] Create `handlers/extended_thinking_handler.py` (with caching)
- [ ] Add TTL cache implementation
- [ ] Implement fallback logic
- [ ] Add cost tracking
- [ ] Add to container
- [ ] Test critical dependency alerts
- [ ] Verify AI recommendations quality

### Phase 3: Lock Manager
- [ ] Create `lib/refactoring_lock_manager.py` (multi-backend)
- [ ] Implement MemoryLockBackend
- [ ] Implement FileLockBackend (fcntl)
- [ ] Add to container (Singleton)
- [ ] Test TTL enforcement
- [ ] Test lock contention scenarios
- [ ] Test file-based locks in CI environment

### Infrastructure: Feature Flags & Config
- [ ] Create `lib/feature_flags.py`
- [ ] Create `.claude/config/performance_slos.yaml`
- [ ] Create `.env.development`, `.env.ci`, `.env.production`
- [ ] Implement SLO monitoring hooks
- [ ] Test feature flag toggling

### Phase 4: Refactoring Skill
- [ ] Create `.claude/skills/refactoring-safety/` structure
- [ ] Write SKILL.md (Level 1+2)
- [ ] Create CLI scripts (analyze_impact.py, rollback.py)
- [ ] Implement dry-run mode
- [ ] Write reference documentation (4 files)
- [ ] Create refactoring templates (3 YAML)

### Phase 5: Dashboard
- [ ] Create `DependencyEventViewer.vue`
- [ ] Add to App.vue
- [ ] Implement EventAggregator in server
- [ ] Add batch grouping & filters
- [ ] Test real-time event display
- [ ] Verify batch aggregation

### Phase 6: Batch Refactoring
- [ ] Create `handlers/batch_refactoring_handler.py`
- [ ] Integrate with lock manager
- [ ] Add to container
- [ ] Test parallel event emission
- [ ] Test lock coordination
- [ ] Benchmark performance

### Phase 7: File Watcher (Optional)
- [ ] Create `tools/dependency_watch_daemon.py`
- [ ] Integrate watchdog library
- [ ] Implement debounce logic
- [ ] Prevent resource leaks (task pool)
- [ ] Test file watcher ‚Üí event flow

### Testing & Validation
- [ ] Unit tests (all phases)
- [ ] Integration tests (event flow)
- [ ] E2E workflow tests (5 scenarios)
- [ ] Chaos engineering tests (4 scenarios)
- [ ] Performance benchmarks
- [ ] SLO compliance verification
- [ ] Rollback procedures verification

---

## üéì ÌïôÏäµ ÏûêÎ£å

### Event-Driven Architecture
- Martin Fowler: Event-Driven Architecture
- Reactive Manifesto: https://www.reactivemanifesto.org
- Pub/Sub pattern best practices

### Dependency Analysis
- NetworkX documentation: https://networkx.org
- AST module: https://docs.python.org/3/library/ast.html
- Software metrics (Coupling, Instability)

### Observability
- Disler's patterns: claude-code-hooks-multi-agent-observability
- Real-time WebSocket patterns
- Event aggregation strategies

### Concurrency
- asyncio Lock patterns
- TTL-based resource management
- Lock-free algorithms

---

## üìû Î¨∏Ïùò Î∞è ÏßÄÏõê

**ÌîÑÎ°úÏ†ùÌä∏ Í¥ÄÎ†® Î¨∏Ïùò**:
- GitHub Issues: `/home/kc-palantir/math/.github/issues`
- ÎÇ¥Î∂Ä Î¨∏ÏÑú: `.claude/CLAUDE.md`

## üéØ Senior Developer Review Summary

### ‚≠ê Overall Assessment: 4.4/5 (Production-Ready)

Ïù¥ Í≥ÑÌöçÏùÄ ÏãúÎãàÏñ¥ Í∞úÎ∞úÏûê Í≤ÄÌÜ†Î•º ÌÜµÍ≥ºÌñàÏúºÎ©∞, 3Í∞ú Critical Issues Ìï¥Í≤∞ Î∞è ÎàÑÎùΩ Í∏∞Îä• Î≥¥ÏôÑÏùÑ ÏôÑÎ£åÌñàÏäµÎãàÎã§.

### ‚úÖ Ìï¥Í≤∞Îêú Critical Issues

| Issue | Î¨∏Ï†ú | Ìï¥Í≤∞Ï±Ö | Íµ¨ÌòÑ Phase |
|-------|------|--------|-----------|
| **#1** | HookÏùò Async Error Propagation | Critical pathÎäî blocking, non-criticalÏùÄ fire-and-forget | Phase 1.3 |
| **#2** | Lock Manager Î∂ÑÏÇ∞ ÌôòÍ≤Ω ÎØ∏ÏßÄÏõê | FileLockBackend (fcntl) Ï∂îÍ∞Ä | Phase 3.1 |
| **#3** | Extended Thinking API ÎπÑÏö©/ÏßÄÏó∞ | TTL Ï∫êÏã± + fallback logic | Phase 2.1 |

### üìù Ï∂îÍ∞ÄÎêú Í∏∞Îä•

| Í∏∞Îä• | ÏÑ§Î™Ö | Íµ¨ÌòÑ ÏúÑÏπò |
|------|------|-----------|
| **Feature Flags** | ÌôòÍ≤ΩÎ≥Ñ ÏÑ§Ï†ï Í¥ÄÎ¶¨ | `lib/feature_flags.py` |
| **Performance SLOs** | ÏÑ±Îä• Î™©Ìëú Î∞è Î™®ÎãàÌÑ∞ÎßÅ | `.claude/config/performance_slos.yaml` |
| **Rollback Plan** | ÏûêÎèô/ÏàòÎèô Î°§Î∞± ÏßÄÏõê | `scripts/rollback_refactoring.py` |
| **Dry-run Mode** | Ïã§Ï†ú ÏàòÏ†ï ÏóÜÏù¥ Î∂ÑÏÑùÎßå | Feature flag `DRY_RUN_MODE` |
| **Baseline Comparison** | Ïù¥Ï†Ñ/ÌòÑÏû¨ ÏùòÏ°¥ÏÑ± ÎπÑÍµê | Phase 0.3 |
| **Cost Tracking** | Extended Thinking ÎπÑÏö© Ï∂îÏ†Å | Phase 2.1 |

### üìä ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÉÄÏûÑÎùºÏù∏

| Î≤ÑÏ†Ñ | ÏòàÏÉÅ ÏùºÏàò | Ïã§Ï†ú Ï°∞Ï†ï | Î≤ÑÌçº |
|------|----------|----------|------|
| v2.0 | 12Ïùº | - | - |
| v3.0 | 14Ïùº | - | - |
| **Final (Senior Review)** | **19Ïùº** | +5Ïùº | **+27%** |

**Ï°∞Ï†ï ÏÇ¨Ïú†**:
- Phase 0: +1Ïùº (comparison mode, git hooks)
- Phase 2: +1Ïùº (caching, fallback)
- Phase 3: +1Ïùº (file-based backend)
- Phase 5: +1Ïùº (aggregation)
- Infrastructure: +2Ïùº (feature flags, SLOs)

### üéØ Go/No-Go Decision

**‚úÖ GO - Approved with Conditions Met**

**ÏäπÏù∏ Ï°∞Í±¥ (Î™®Îëê Ï∂©Ï°±)**:
1. ‚úÖ Critical Issues Ìï¥Í≤∞ (1-3Î≤à)
2. ‚úÖ Î¨∏ÏÑú Î≥¥ÏôÑ (Rollback, SLOs, Feature flags)
3. ‚úÖ Ïö∞ÏÑ†ÏàúÏúÑ Ï°∞Ï†ï (Lock Manager Week 1Î°ú Ïù¥Îèô)
4. ‚úÖ ÌÉÄÏûÑÎùºÏù∏ Ïû¨Ï°∞Ï†ï (19Ïùº)

### üìã Ï£ºÏöî Í∞úÏÑ†ÏÇ¨Ìï≠

**ÏïÑÌÇ§ÌÖçÏ≤ò**:
- Event-driven Ìå®ÌÑ¥ ÏùºÍ¥ÄÏÑ± Ïú†ÏßÄ
- Î∂ÑÏÇ∞ ÌôòÍ≤Ω ÏßÄÏõê (file-based locks)
- Feature flag Í∏∞Î∞ò Ï†êÏßÑÏ†Å Î∞∞Ìè¨

**ÏïàÏ†ÑÏÑ±**:
- Critical path blocking (50+ nodes)
- Rollback automation
- SLO monitoring & alerting

**ÎπÑÏö© ÏµúÏ†ÅÌôî**:
- Extended Thinking TTL Ï∫êÏã± (5Î∂Ñ)
- 80% Ï∫êÏãú hit rate Î™©Ìëú
- SessionÎãπ $1 ÎπÑÏö© Ï†úÌïú

**Í∞ÄÏãúÏÑ±**:
- Real-time dashboard integration
- Event aggregation (5Ï¥à window)
- Batch grouping & filtering

### üöÄ Ready for Implementation

Ïù¥ Í≥ÑÌöçÏùÄ **ÌîÑÎ°úÎçïÏÖò ÌôòÍ≤Ω Î∞∞Ìè¨ Ï§ÄÎπÑ ÏôÑÎ£å** ÏÉÅÌÉúÏûÖÎãàÎã§.

Îã§Ïùå Îã®Í≥Ñ:
1. Week 1 Sprint ÏãúÏûë (Phase 0-1-3)
2. Daily standupÏúºÎ°ú ÏßÑÌñâ ÏÉÅÌô© Ï∂îÏ†Å
3. Phase 0 ÏôÑÎ£å ÌõÑ baseline Í≥µÏú†
4. Phase 2 ÏôÑÎ£å ÌõÑ ÎπÑÏö© Î©îÌä∏Î¶≠ Í≤ÄÌÜ†

---

**Î¨∏ÏÑú Î≤ÑÏ†Ñ**:
- **ÏµúÏ¢Ö Í≥ÑÌöç Î≤ÑÏ†Ñ**: 2.0 (Senior Review Approved)
- **Í∏∞Î∞ò**: v3.0 + Gap Analysis + Senior Developer Review
- **ÏµúÏ¢Ö ÏàòÏ†ï**: 2025-10-17
- **Í≤ÄÌÜ†Ïûê**: Senior Developer
- **Îã§Ïùå Î¶¨Î∑∞**: Phase 0-1 ÏôÑÎ£å ÌõÑ (Week 1 Ï¢ÖÎ£å)
- **ÏäπÏù∏ ÏÉÅÌÉú**: ‚úÖ Approved with all revisions implemented

---

**Ïù¥ Î¨∏ÏÑúÎäî Îã§ÏùåÏùÑ ÌÜµÌï©Ìïú ÏµúÏ¢Ö ÌîÑÎ°úÎçïÏÖò Ï§ÄÎπÑ Ïã§Ìñâ Í≥ÑÌöçÏûÖÎãàÎã§**:
1. DEPENDENCY-REFACTORING-IMPROVEMENT-PLAN.md (v2.0) - Ï¥àÍ∏∞ ÏÑ§Í≥Ñ
2. DEPENDENCY-REFACTORING-IMPROVEMENT-PLAN-v3.md - Event-driven Ïû¨ÏÑ§Í≥Ñ
3. GAP-ANALYSIS-SUMMARY.md - Î¶¨Ïä§ÌÅ¨ Î∂ÑÏÑù Î∞è Î≥¥ÏôÑ
4. Senior Developer Review - Critical Issues Ìï¥Í≤∞ Î∞è ÏµúÏ¢Ö Í≤ÄÏ¶ù
